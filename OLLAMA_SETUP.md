# ğŸ¤– ConfiguraciÃ³n de Ollama para PersAcc

## ğŸ“¥ Paso 1: Instalar Ollama

1. **Descarga Ollama**:
   - Ve a: https://ollama.com/download
   - Descarga el instalador para Windows
   - Ejecuta el instalador (muy simple, siguiente â†’ siguiente â†’ finalizar)

2. **Verifica la instalaciÃ³n**:
   ```powershell
   ollama --version
   ```
   
   Si ves el nÃºmero de versiÃ³n, Â¡estÃ¡ instalado! ğŸ‰

## ğŸ¯ Paso 2: Descargar un Modelo

Elige **UNO** de estos modelos segÃºn tus recursos:

### ğŸª¶ Light (Recomendado para empezar)
```powershell
ollama pull tinyllama
```
- **TamaÃ±o**: ~0.6GB
- **RAM necesaria**: 4GB
- **Calidad**: â­â­
- **Velocidad**: Muy rÃ¡pido

### ğŸƒ Standard (Equilibrado - Recomendado)
```powershell
ollama pull phi3
```
- **TamaÃ±o**: ~2.3GB
- **RAM necesaria**: 6GB
- **Calidad**: â­â­â­
- **Velocidad**: RÃ¡pido

### ğŸ’ª Quality (Mejor anÃ¡lisis)
```powershell
ollama pull mistral
```
- **TamaÃ±o**: ~4.1GB
- **RAM necesaria**: 8GB
- **Calidad**: â­â­â­â­
- **Velocidad**: Moderado

### ğŸš€ Premium (MÃ¡xima calidad)
```powershell
ollama pull llama3
```
- **TamaÃ±o**: ~4.7GB
- **RAM necesaria**: 12GB
- **Calidad**: â­â­â­â­â­
- **Velocidad**: MÃ¡s lento

## âš™ï¸ Paso 3: Configurar PersAcc

Edita `data/config.json` y ajusta el modelo segÃºn lo que descargaste:

```json
{
  "llm": {
    "enabled": true,
    "model_tier": "standard",  // Opciones: "light", "standard", "quality", "premium"
    "max_tokens": 300
  }
}
```

## ğŸš€ Paso 4: Ejecutar la App

1. **Instala las dependencias**:
   ```powershell
   pip install -r requirements.txt
   ```

2. **Ejecuta Streamlit**:
   ```powershell
   streamlit run app.py
   ```

3. **Usa el anÃ¡lisis IA**:
   - Ve a **HistÃ³rico**
   - Selecciona un aÃ±o con datos
   - Expande **"ğŸ¤– AnÃ¡lisis Financiero IA"**
   - Haz clic en **"Generar AnÃ¡lisis"**

## ğŸ” Verificar que Ollama estÃ¡ corriendo

Si el botÃ³n te da error de que Ollama no estÃ¡ corriendo:

```powershell
# Ver si Ollama estÃ¡ corriendo
Get-Process ollama -ErrorAction SilentlyContinue

# Si no estÃ¡ corriendo, inÃ­cialo manualmente
ollama serve
```

Ollama deberÃ­a iniciarse automÃ¡ticamente al instalar, pero si no:
- Busca "Ollama" en el menÃº inicio y ejecÃºtalo
- O reinicia tu PC

## ğŸ¨ Cambiar de Modelo

Para probar otro modelo:

1. Descarga el nuevo modelo:
   ```powershell
   ollama pull mistral
   ```

2. Cambia `model_tier` en `config.json` a `"quality"`

3. Â¡Listo! La prÃ³xima vez que generes anÃ¡lisis usarÃ¡ el nuevo modelo

## ğŸ§¹ GestiÃ³n de Modelos

```powershell
# Ver modelos descargados
ollama list

# Eliminar un modelo que no uses
ollama rm tinyllama

# Ver cuÃ¡nto espacio ocupan
ollama list
```

## â“ Troubleshooting

### "Ollama no estÃ¡ ejecutÃ¡ndose"
```powershell
ollama serve
```

### "Modelo no descargado"
```powershell
ollama pull [nombre-modelo]
```

### Ver logs de Ollama
```powershell
# Windows: busca en
%LOCALAPPDATA%\Ollama\logs
```

### Actualizar Ollama
- Descarga la Ãºltima versiÃ³n desde https://ollama.com/download
- InstÃ¡lala sobre la existente

## ğŸ“Š Modelos Disponibles

Puedes ver todos los modelos disponibles en: https://ollama.com/library

Algunos populares para anÃ¡lisis financiero:
- `tinyllama` - SÃºper ligero y rÃ¡pido
- `phi3` - Excelente equilibrio
- `mistral` - Alta calidad
- `llama3` - Top tier
- `gemma2` - Alternativa de Google

## ğŸ’¡ Tips

1. **Primer anÃ¡lisis**: El primer anÃ¡lisis de cada sesiÃ³n puede tardar 5-10 segundos mientras Ollama carga el modelo
2. **Siguientes anÃ¡lisis**: Son mÃ¡s rÃ¡pidos (2-3 segundos)
3. **Modelos pequeÃ±os**: Perfectos para anÃ¡lisis breves y directos
4. **Modelos grandes**: Mejor para anÃ¡lisis mÃ¡s profundos y contextuales
5. **RAM**: Si tu PC se pone lento, usa un modelo mÃ¡s pequeÃ±o

## ğŸ‰ Â¡Listo!

Ahora tienes IA local funcionando en tu app de finanzas personales, completamente offline y gratuita.
